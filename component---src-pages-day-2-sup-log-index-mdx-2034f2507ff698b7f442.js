(window.webpackJsonp=window.webpackJsonp||[]).push([[49],{"013z":function(e,t,o){"use strict";o("KKXr"),o("pIFo");var n=o("pOBw"),a=o("q1tI"),i=o.n(a),s=o("NmYn"),l=o.n(s),r=o("OKom"),c=o("k4MR"),u=o("TSYQ"),d=o.n(u),p=o("QH2O"),b=o("qKvR"),h=function(e){var t,o=e.title,n=e.tabs,a=void 0===n?[]:n;return Object(b.b)("div",{className:d()(p.pageHeader,(t={},t[p.withTabs]=a.length,t))},Object(b.b)("div",{className:"bx--grid"},Object(b.b)("div",{className:"bx--row"},Object(b.b)("div",{className:"bx--col-lg-12"},Object(b.b)("h1",{id:"page-title",className:p.text},o)))))},g=o("pEPl"),m=o("BAC9"),f=function(e){var t=e.relativePagePath,o=e.repository,n=g.data.site.siteMetadata.repository,a=o||n,i=a.baseUrl,s=a.subDirectory,l=i+"/edit/"+a.branch+s+"/src/pages"+t;return i?Object(b.b)("div",{className:"bx--row "+m.row},Object(b.b)("div",{className:"bx--col"},Object(b.b)("a",{className:m.link,href:l},"Edit this page on GitHub"))):null},O=o("FCXl"),y=(o("Oyvg"),o("Wbzz")),w=o("I8xM");var v=function(e){var t,o;function n(){return e.apply(this,arguments)||this}return o=e,(t=n).prototype=Object.create(o.prototype),t.prototype.constructor=t,t.__proto__=o,n.prototype.render=function(){var e=this.props,t=e.tabs,o=e.slug,n=o.split("/").filter(Boolean).slice(-1)[0],a=t.map((function(e){var t,a=l()(e,{lower:!0}),i=a===n,s=new RegExp(n+"(?!-)"),r=o.replace(s,a);return Object(b.b)("li",{key:e,className:d()((t={},t[w.selectedItem]=i,t),w.listItem)},Object(b.b)(y.Link,{className:w.link,to:""+r},e))}));return Object(b.b)("div",{className:w.tabsContainer},Object(b.b)("div",{className:"bx--grid"},Object(b.b)("div",{className:"bx--row"},Object(b.b)("div",{className:"bx--col-lg-12 bx--col-no-gutter"},Object(b.b)("nav",null,Object(b.b)("ul",{className:w.list},a))))))},n}(i.a.Component),j=o("MjG9");t.a=function(e){var t=e.pageContext,o=e.children,a=e.location,i=e.Title,s=t.frontmatter,u=void 0===s?{}:s,d=t.relativePagePath,p=t.titleType,g=u.tabs,m=u.title,y=u.theme,w=u.description,S=u.keywords,N=n.data.site.pathPrefix,_=N?a.pathname.replace(N,""):a.pathname,A=g?_.split("/").filter(Boolean).slice(-1)[0]||l()(g[0],{lower:!0}):"";return Object(b.b)(c.a,{tabs:g,homepage:!1,theme:y,pageTitle:m,pageDescription:w,pageKeywords:S,titleType:p},Object(b.b)(h,{title:i?Object(b.b)(i,null):m,label:"label",tabs:g}),g&&Object(b.b)(v,{slug:_,tabs:g,currentTab:A}),Object(b.b)(j.a,{padded:!0},o,Object(b.b)(f,{relativePagePath:d})),Object(b.b)(O.a,{pageContext:t,location:a,slug:_,tabs:g,currentTab:A}),Object(b.b)(r.a,null))}},pEPl:function(e){e.exports=JSON.parse('{"data":{"site":{"siteMetadata":{"repository":{"baseUrl":"https://github.com/ibm-cloud-architecture/cloudpak8s","subDirectory":"/","branch":"master"}}}}}')},pOBw:function(e){e.exports=JSON.parse('{"data":{"site":{"pathPrefix":""}}}')},wKoB:function(e,t,o){"use strict";o.r(t),o.d(t,"_frontmatter",(function(){return s})),o.d(t,"default",(function(){return c}));o("91GP"),o("rGqo"),o("yt8O"),o("Btvt"),o("RW0V"),o("q1tI");var n=o("7ljp"),a=o("013z");o("qKvR");function i(){return(i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var o=arguments[t];for(var n in o)Object.prototype.hasOwnProperty.call(o,n)&&(e[n]=o[n])}return e}).apply(this,arguments)}var s={},l={_frontmatter:s},r=a.a;function c(e){var t=e.components,o=function(e,t){if(null==e)return{};var o,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)o=i[n],t.indexOf(o)>=0||(a[o]=e[o]);return a}(e,["components"]);return Object(n.b)(r,i({},l,o,{components:t,mdxType:"MDXLayout"}),Object(n.b)("h1",null,"Logging Solutions on IBM Cloud"),Object(n.b)("p",null,"The logs such as system log and application log are useful for debugging and monitoring cluster activity. Most of container platform supports a logging solution including OpenShift Container Platform.  "),Object(n.b)("p",null,"By default, logs are generated and written locally.  Several logging solutions are available to collect, forward, and view the logs in OpenShift on IBM Cloud.  "),Object(n.b)("h2",null,"Choosing a logging solution"),Object(n.b)("p",null,"You can choose your logging solution based on which cluster components you need to collect logs for. There are several logs produced in different components in OpenShift as follow:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"Application log"),Object(n.b)("li",{parentName:"ul"},"OpenShift Master log"),Object(n.b)("li",{parentName:"ul"},"OpenShift Worker Nodes/Pods log  ")),Object(n.b)("p",null,"A common implementation is to choose a logging service that you prefer based on its analysis and interface capabilities, such as IBM Log Analysis with LogDNA or Built-in OpenShift Logging functions.  "),Object(n.b)("p",null,"You can then use IBM Cloud Activity Tracker with LogDNA to audit user activity in the cluster.  We will discuss it in the later section.  "),Object(n.b)("h2",null,"Application Log"),Object(n.b)("p",null,"You would generate your applications’ logs for several purposes.  If your applications encounter errors, it would log those errors and those logs may be useful for debugging.  Every application may write logs in different places with different format.  Therefore, you may need to consider how to take care of those logs.  "),Object(n.b)("p",null,"With OpenShift cluster, you would aggregate your applications logs into a central location since your applications may run on several different pods/nodes.  For example, you would forward your application logs to LogDNA and let LogDNA manage those logs.  "),Object(n.b)("h2",null,"OpenShift Master log"),Object(n.b)("p",null,"To collect, forward, and view logs for your cluster’s Kubernetes master, you can take a snapshot of the Master logs at any point in time to collect in an IBM Cloud Object Storage bucket. The snapshot includes anything that is sent through the API server, such as pod scheduling, deployments, or RBAC policies.",Object(n.b)("br",{parentName:"p"}),"\n",Object(n.b)("a",i({parentName:"p"},{href:"https://cloud.ibm.com/docs/containers?topic=containers-health#collect_master"}),"https://cloud.ibm.com/docs/containers?topic=containers-health#collect_master"),"  "),Object(n.b)("h2",null,"OpenShift Worker Nodes/Pods log"),Object(n.b)("p",null,"There are several ways to check the Worker nodes/pods log.  You can use the CLI to check the logs of an individual OpenShift pod.  You can also use GUI with several different solutions such as OpenShift Web Console, IBM Log Analysis with LogDNA, and external servers with Fluend.  We will take a look at those solutions in the following sections.  "),Object(n.b)("h2",null,"Using the CLI for OpenShift Pod logs"),Object(n.b)("p",null,"You can run the command “oc logs ",Object(n.b)("em",{parentName:"p"},"pod_name")," ” to see the OpenShift pod logs as below."),Object(n.b)("pre",null,Object(n.b)("code",i({parentName:"pre"},{}),'$ oc logs httpd-1-htzpn\n=> sourcing 10-set-mpm.sh ...\n=> sourcing 20-copy-config.sh ...\n=> sourcing 40-ssl-certs.sh ...\nAH00558: httpd: Could not reliably determine the server\'s fully qualified domain name, using 172.30.48.85. Set the \'ServerName\' directive globally to suppress this message\n[Sat Oct 12 14:18:15.760713 2019] [ssl:warn] [pid 1] AH01909: 172.30.48.85:8443:0 server certificate does NOT include an ID which matches the server name\n[Sat Oct 12 14:18:15.761653 2019] [:notice] [pid 1] ModSecurity for Apache/2.9.3 (http://www.modsecurity.org/) configured.\n[Sat Oct 12 14:18:15.761668 2019] [:notice] [pid 1] ModSecurity: APR compiled version="1.4.8"; loaded version="1.4.8"\n[Sat Oct 12 14:18:15.761673 2019] [:notice] [pid 1] ModSecurity: PCRE compiled version="8.32 "; loaded version="8.32 2012-11-30"\n[Sat Oct 12 14:18:15.761680 2019] [:notice] [pid 1] ModSecurity: LUA compiled version="Lua 5.1"\n[Sat Oct 12 14:18:15.761684 2019] [:notice] [pid 1] ModSecurity: YAJL compiled version="2.0.4"\n[Sat Oct 12 14:18:15.761688 2019] [:notice] [pid 1] ModSecurity: LIBXML compiled version="2.9.1"\n[Sat Oct 12 14:18:15.761693 2019] [:notice] [pid 1] ModSecurity: Status engine is currently disabled, enable it by set SecStatusEngine to On.\nAH00558: httpd: Could not reliably determine the server\'s fully qualified domain name, using 172.30.48.85. Set the \'ServerName\' directive globally to suppress this message\n[Sat Oct 12 14:18:15.854039 2019] [ssl:warn] [pid 1] AH01909: 172.30.48.85:8443:0 server certificate does NOT include an ID which matches the server name\n[Sat Oct 12 14:18:15.854206 2019] [http2:warn] [pid 1] AH10034: The mpm module (prefork.c) is not supported by mod_http2. The mpm determines how things are processed in your server. HTTP/2 has more demands in this regard and the currently selected mpm will just not do. This is an advisory warning. Your server will continue to work, but the HTTP/2 protocol will be inactive.\n[Sat Oct 12 14:18:15.854933 2019] [lbmethod_heartbeat:notice] [pid 1] AH02282: No slotmem from mod_heartmonitor\n[Sat Oct 12 14:18:15.859059 2019] [mpm_prefork:notice] [pid 1] AH00163: Apache/2.4.34 (Red Hat) OpenSSL/1.0.2k-fips configured -- resuming normal operations\n[Sat Oct 12 14:18:15.859089 2019] [core:notice] [pid 1] AH00094: Command line: \'httpd -D FOREGROUND\'\n[Sat Oct 12 14:18:36.654769 2019] [autoindex:error] [pid 38] [client 172.30.75.75:42370] AH01276: Cannot serve directory /opt/rh/httpd24/root/var/www/html/: No matching DirectoryIndex (index.html) found, and server-generated directory index forbidden by Options directive\n172.30.75.75 - - [12/Oct/2019:14:18:36 +0000] "GET / HTTP/1.1" 403 3985 "-" "curl/7.54.0"\n$\n')),Object(n.b)("p",null,"You can use the —follow option with “oc logs” command to monitor the logs for that pod continually.  "),Object(n.b)("h2",null,"Using OpenShift Web Console for Pod logs on IBM Cloud"),Object(n.b)("p",null,"OpenShift provides a capability to see Pod’s logs on the Web Console as shown in below.  "),Object(n.b)("img",{src:"/assets/img/day2/ibmcloud_ocp_dashboard_pod_logs.png",alt:"ibmcloud_ocp_dashboard_pod_logs"}),Object(n.b)("p",null,"You can see the same logs with the oc command as shown in the later section. Using the CLI for OpenShift Pod logs.  "),Object(n.b)("h2",null,"EFK stack in OpenShift is NOT supported on IBM Cloud"),Object(n.b)("p",null,"Unlike other environment, the EFK stack such as Elasticsearch, Fluentd, and Kibana as shown in Figure 12 is NOT supported on IBM Cloud because you cannot modify the default configuration of the Red Hat OpenShift on IBM Cloud cluster.  "),Object(n.b)("p",null,Object(n.b)("img",i({parentName:"p"},{src:"/assets/img/day2/ibmcloud_efk_not_supported.png",alt:"ibmcloud_efk_not_supported"})),"  "),Object(n.b)("p",null,"Therefore, the EFK in the OpenShift on IBM Cloud is not an option for the logging solution.  "),Object(n.b)("h2",null,"Using IBM Log Analysis with LogDNA"),Object(n.b)("p",null,"You can manage container logs by deploying LogDNA agent to OpenShift cluster. The agent collects logs with the extension .log and extensionless files that are stored in the /var/log directory of pods from all namespaces, including kube-system. The agent then forwards the logs to the IBM Log Analysis with LogDNA service as shown in below.  "),Object(n.b)("img",{src:"/assets/img/day2/ibmcloud_logdna.png",alt:"ibmcloud_logdna"}),Object(n.b)("p",null,"The sample output on the LogDNA dashboard is shown in below.  "),Object(n.b)("img",{src:"/assets/img/day2/ibmcloud_logdna_dashboard.png",alt:"ibmcloud_logdna_dashboard"}),Object(n.b)("h2",null,"Using Fluentd with an external server"),Object(n.b)("p",null,"To collect, forward, and view logs for a cluster component, you can create a logging configuration by using Fluentd. When you create a logging configuration, the Fluentd cluster component collects logs from the paths for a specified source. Fluentd can then forward these logs to an external server that accepts a syslog protocol as shown in below."),Object(n.b)("p",null,Object(n.b)("img",i({parentName:"p"},{src:"/assets/img/day2/ibmcloud_external_efk.png",alt:"ibmcloud_external_efk"})),"  "),Object(n.b)("h2",null,"Understanding options for logging on IBM Cloud"),Object(n.b)("p",null,"To help understand when to use the built-in OpenShift tools or IBM Cloud Log Analysis with LogDNA, you can review the following table.",Object(n.b)("br",{parentName:"p"}),"\n","Note that this is for OpenShift 3.11 statement. With OpenShift 4.2, this will be updated."),Object(n.b)("table",null,Object(n.b)("thead",{parentName:"table"},Object(n.b)("tr",{parentName:"thead"},Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"OpenShift tools:Built-in OpenShift logging tools"),Object(n.b)("th",i({parentName:"tr"},{align:"left"}),"IBM Cloud integrations:IBM Log Analysis with LogDNA"))),Object(n.b)("tbody",{parentName:"table"},Object(n.b)("tr",{parentName:"tbody"},Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"• Built-in view of pod logs in the OpenShift web console.• Built-in pod logs are not configured with persistent storage. You must integrate with a cloud database to back up the logging data and make it highly available and manage the logs yourself.Note: You cannot run the Ansible playbook to deploy the OpenShift Container Platform Elasticsearch, Fluentd, and Kibana (EFK) stack because you cannot modify the default configuration of the Red Hat OpenShift on IBM Cloud cluster."),Object(n.b)("td",i({parentName:"tr"},{align:"left"}),"• Customizable user interface for live streaming of log tailing, real-time troubleshooting, issue alerts, and log archiving.• Aggregated logs across clusters and cloud providers.• Historical access to logs that is based on the plan you choose.• Highly available, scalable, and compliant with industry security standards.• Integrated with IBM Cloud IAM for user access management.• Quick integration with the cluster via a script.• Flexible plans, including a free Lite option.• Log Analysis with DNA can aggregate and manage logs from multiple Kubernetes and OpenShift clusters, several Cloud Databases services, and the Certificate Manager Service.• 3rd party LogDNA agent has to be added to worker nodes to forward logs from pod containers to the IBM Log Analysis with LogDNA service.")))),Object(n.b)("h2",null,"Third-party services or configure your own logging"),Object(n.b)("p",null,"In case you have special requirements for logging, you can set up your own logging solution. Check out third-party logging services such as Splunk that you can add to OpenShift cluster.",Object(n.b)("br",{parentName:"p"}),"\n",Object(n.b)("a",i({parentName:"p"},{href:"https://cloud.ibm.com/docs/containers?topic=containers-supported_integrations#health_services"}),"https://cloud.ibm.com/docs/containers?topic=containers-supported_integrations#health_services"),"  "),Object(n.b)("h1",null,"Solution for Audit Logging"),Object(n.b)("p",null,"You can also collect Kubernetes API audit logs from OpenShift cluster on IBM Cloud and forward them to IBM Log Analysis with LogDNA. To access OpenShift (Kubernetes) audit logs, you must create an audit webhook in the cluster.  "),Object(n.b)("p",null,"The Kubernetes audit system in OpenShift cluster consists of an audit webhook, a log collection service and webserver app, and a logging agent. The webhook collects the Kubernetes API server events from the Master. The log collection service is a Kubernetes ClusterIP service that is created from an image from the public IBM Cloud registry. This service exposes a simple node.js HTTP webserver app that is exposed only on the private network. The webserver app parses the log data from the audit webhook and creates each log as a unique JSON line. Finally, the logging agent forwards the logs from the webserver app to IBM Log Analysis with LogDNA, where you can view the logs.  "),Object(n.b)("p",null,Object(n.b)("a",i({parentName:"p"},{href:"https://cloud.ibm.com/docs/containers?topic=containers-health#webhook_logdna"}),"https://cloud.ibm.com/docs/containers?topic=containers-health#webhook_logdna")),Object(n.b)("img",{src:"/assets/img/day2/ibmcloud_audit_log_activity_tracker.png",alt:"ibmcloud_audit_log_activity_tracker"}),Object(n.b)("p",null,"Sample output of audit log events on the Activity Tracker dashboard is shown in below.  "),Object(n.b)("img",{src:"/assets/img/day2/ibmcloud_activity_tracker_dashboard.png",alt:"ibmcloud_activity_tracker_dashboard"}))}c.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-day-2-sup-log-index-mdx-2034f2507ff698b7f442.js.map